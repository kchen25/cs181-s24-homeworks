{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c97605-bfe4-4ebc-9182-9562f0b18c4a",
   "metadata": {},
   "source": [
    "# Problem 2 - Policy and Value Iteration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c1b2b",
   "metadata": {},
   "source": [
    "## Util methods to represent the grid (do not modify)\n",
    "#### You do not need modify any of these methods to complete Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28eded-7a33-49e3-be11-8c2d8ddee092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maze state is represented as a 2-element NumPy array: (Y, X). Increasing Y is South.\n",
    "grid = [\n",
    "    'o.x.u',\n",
    "    '..y..',\n",
    "    '..y.*',\n",
    "    '..z..']\n",
    "topology = np.array([list(row) for row in grid])\n",
    "flat_topology = topology.ravel()\n",
    "rewards = {'.': 0, '*': 50, 'o': 4, 'u': 20, 'x': -10, 'y': -50,'z': -20}\n",
    "\n",
    "row_count = len(grid) # 4\n",
    "col_count = len(grid[0]) # 5\n",
    "shape = (row_count, col_count) # (4, 5)\n",
    "\n",
    "# Possible actions, expressed as (delta-y, delta-x)\n",
    "directions=\"NSEW\"\n",
    "maze_actions = {\n",
    "    'N': np.array([-1, 0]),\n",
    "    'S': np.array([1, 0]),\n",
    "    'E': np.array([0, 1]),\n",
    "    'W': np.array([0, -1]),\n",
    "}\n",
    "actions = [maze_actions[direction] for direction in directions]\n",
    "\n",
    "\n",
    "# Returns true if pos (y,x) is out of bounds\n",
    "def is_wall(pos):\n",
    "    (y, x) = pos\n",
    "    return (y < 0 or y >= row_count or x < 0 or x >= col_count)\n",
    "\n",
    "# Input is a flattened state, returns the reward at that state\n",
    "def get_reward(state):\n",
    "    assert (state in range(num_states)), f\"get_reward: State was not an integer representing an in-bounds state ({state} was given)\"\n",
    "    return rewards.get(flat_topology[int(state)])\n",
    "\n",
    "# Input is a flattened state, returns the unflattened representation of the state\n",
    "def unflatten_index(flattened_index):\n",
    "    return np.unravel_index(flattened_index, shape)\n",
    "\n",
    "# Input state is an unflattened position and action is an index into the actions[] array\n",
    "# Returns a tuple containing the new position of taking the action from the state\n",
    "def move(state, action):\n",
    "    return tuple((state + actions[action]).reshape(1, -1)[0])\n",
    "\n",
    "# Returns an array of the \"side states\" when taking action beginning at unflattened position state\n",
    "# Does not return states which are out of bounds\n",
    "def get_side_states(action, state):\n",
    "    side_states = []\n",
    "    \n",
    "    if action == 0 or action == 1:\n",
    "        if not is_wall(move(state, 3)):\n",
    "            side_states.append(move(state, 3))\n",
    "        if not is_wall(move(state, 2)):\n",
    "            side_states.append(move(state, 2))\n",
    "    elif action == 2 or action == 3:\n",
    "        if not is_wall(move(state, 0)):\n",
    "            side_states.append(move(state, 0))\n",
    "        if not is_wall(move(state, 1)):\n",
    "            side_states.append(move(state, 1))\n",
    "            \n",
    "    return side_states\n",
    "    \n",
    "# Inputs s1, s2 are flattened states, action represents an index into the actions array\n",
    "# Returns p(s2 | s1, action)\n",
    "def get_transition_prob(s1, action, s2):\n",
    "    # Check the inputs are valid\n",
    "    assert (action in [0,1,2,3]), f\"get_transition_prob: Action needs to be an integer in [0,1,2,3], but {action} was given\"\n",
    "    assert (s1 in range(num_states)), f\"get_transition_prob: Input s1 was not an integer representing an in-bounds state ({s1} was given)\"\n",
    "    assert (s2 in range(num_states)), f\"get_transition_prob: Input s2 was not an integer representing an in-bounds state ({s2} was given)\"\n",
    "    \n",
    "    state1 = unflatten_index(int(s1))\n",
    "    state2 = unflatten_index(int(s2))\n",
    "    action = int(action)\n",
    "\n",
    "    new_state = move(state1, action)\n",
    "\n",
    "    sstates = get_side_states(action, state1)\n",
    "    succeed_prb = 0.8\n",
    "    slip_prb = 0.1\n",
    "\n",
    "    # One of the side states was a wall: adjust probabilities accordingly.\n",
    "    if len(sstates) == 1:\n",
    "        succeed_prb = 0.9\n",
    "\n",
    "    if is_wall(new_state):\n",
    "        if(state1 == state2):\n",
    "            return succeed_prb\n",
    "    else:\n",
    "        if(state2 == new_state):\n",
    "            return succeed_prb\n",
    "\n",
    "    # Oherwise, check if state2 is on either side\n",
    "    for side_state in sstates:\n",
    "        if(state2 == side_state):\n",
    "            return slip_prb\n",
    "\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8f043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE PLOTS (do not modify)\n",
    "\n",
    "# Util to draw the value function V as numbers on a plot.\n",
    "def make_value_plot(V):\n",
    "    # Useful stats for the plot\n",
    "    value_function = np.reshape(V, shape)\n",
    "\n",
    "    # Write the value on top of each square\n",
    "    indx, indy = np.arange(row_count), np.arange(col_count)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(value_function, interpolation='none', cmap= plt.get_cmap('coolwarm_r'))\n",
    "\n",
    "    for s in range(len(V)):\n",
    "        val = V[s]\n",
    "        (xval, yval) = unflatten_index(s)\n",
    "        t = \"%.2f\"%(val,) # format value with 1 decimal point\n",
    "        ax.text(yval, xval, t, color='black', va='center', ha='center', size=15)\n",
    "        \n",
    "\n",
    "# Util to draw the policy pi as arrows on a plot.\n",
    "def make_policy_plot(pi, iter_type, iter_num):\n",
    "    # Useful stats for the plot\n",
    "    row_count = len(grid)\n",
    "    col_count = len(grid[0])\n",
    "    policy_function = np.reshape(pi, shape)\n",
    "\n",
    "    for row in range(row_count):\n",
    "        for col in range(col_count):\n",
    "            if policy_function[row,col] == 0:\n",
    "                dx = 0; dy = -.5\n",
    "            if policy_function[row,col] == 1:\n",
    "                dx = 0; dy = .5\n",
    "            if policy_function[row,col] == 2:\n",
    "                dx = .5; dy = 0\n",
    "            if policy_function[row,col] == 3:\n",
    "                dx = -.5; dy = 0\n",
    "            plt.arrow( col , row , dx , dy , shape='full', fc='w' , ec='gray' , lw=1., length_includes_head=True, head_width=.1 )\n",
    "    plt.title(iter_type + ' Iteration, i = ' + str(iter_num))\n",
    "    # plt.savefig(iter_type + '_' + str(iter_num) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c612666",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(181)\n",
    "VALUE_ITER = 'Value'\n",
    "POLICY_ITER = 'Policy'\n",
    "\n",
    "num_states = shape[0] * shape[1] # num_states = 20\n",
    "num_actions = len(actions) # num_actions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6a813",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "Below you will implement policy and value iteration.\n",
    "\n",
    "A state is represented as an integer from ``0`` to ``num_states - 1``\n",
    "<br>\n",
    "An action is represented as an integer in ``[0,1,2,3]``, which represents the four cardinal directions [N,S,E,W]\n",
    "\n",
    "Each state has a reward associated with it. The agent gains the reward of a state when it takes an action at that state, not immediately upon entry.\n",
    "\n",
    "``pi`` contains the learned policy at each state, represented by an array of length ``num_states``. In this exercise we will be implementing a deterministic policy, so each state has exactly one action associated with it.\n",
    "<br>\n",
    "ex: [2, 3, 1, 1, 2, 0, 1, 2, 2, 1, 3, 0, 0, 2, 2, 1, 3, 3, 2, 0]\n",
    "\n",
    "``V`` represents the learned value function at each state. Like the above, it is also represented as an array of length ``num_states`` where the entry at index ``i`` represents the value of state ``i``.\n",
    "\n",
    "\n",
    "## Helper methods\n",
    "\n",
    "Recall that when you take an action in Gridworld, you won't always necessarily move in that direction. Instead there is some probability of moving to a state on either side. You do not need to calculate these transition probabilities yourself. Please use the helper functions ``get_transition_prob`` and ``get_reward`` in this file. The method headers are listed below:\n",
    "\n",
    "``get_reward(state):`` Input is a state, output is the reward at that state\n",
    "\n",
    "``get_transition_prob(s1, a, s2):`` Returns the probability of transitioning from state ``s1`` to state ``s2`` upon taking action ``a``.\n",
    "\n",
    "An example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "print(get_reward(14))\n",
    "print(get_transition_prob(16, 0, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8f7a40",
   "metadata": {},
   "source": [
    "## 1a) Policy Evaluation\n",
    "\n",
    "Returns array ``V`` representing the value of policy ``pi`` using discount factor ``gamma``\n",
    "\n",
    "Note: You can do this either closed-form or iteratively. If performed iteratively, please use a convergence tolerance of at least ``0.0001``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2036ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function\n",
    "def policy_evaluation(pi, gamma):\n",
    "    theta = 0.0001\n",
    "    V = np.zeros(num_states)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cc6b5",
   "metadata": {},
   "source": [
    "## 1b) Policy Iteration\n",
    "\n",
    "Now that we have ``V`` computed in 1a), perform **one step** of policy iteration to return the updated policy ``pi_new``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function\n",
    "def update_policy_iteration(V, gamma):\n",
    "    pi_new = np.zeros(num_states)\n",
    "\n",
    "    return pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb96ff",
   "metadata": {},
   "source": [
    "## 2) Value Iteration\n",
    "Given a value function ``V`` and a policy ``pi``, perform **one step** of value iteration and return the updated ``V_new``, ``pi_new``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function\n",
    "def update_value_iteration(V, pi, gamma):\n",
    "    V_new = np.zeros(num_states)\n",
    "    pi_new = np.zeros(num_states)\n",
    "\n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1034d1",
   "metadata": {},
   "source": [
    "# Run code, plot results\n",
    "\n",
    "The ``learn_strategy`` method iteratively runs the one-step methods you wrote in parts 1 and 2, either until the value function converges under <TT>ct</TT> or until a specified number of iterations <TT>max_iter</TT> have elapsed. It will also print out intermediate plots of the learned policy and value function at intervals of <TT>print_every</TT>. The arguments of the function are listed in more detail below:\n",
    "\n",
    "``planning_type`` (<TT>string</TT>): \n",
    "    Specifies whether value or policy iteration is used to learn the strategy.\n",
    "    \n",
    "``max_iter`` (<TT>int</TT>):\n",
    "    The maximum number of iterations (i.e. number of updates) the learning\n",
    "    policy will be run for.\n",
    "    \n",
    "``print_every`` (<TT>int</TT>):\n",
    "    The frequency at which the function will print value and policy plots.\n",
    "    \n",
    "``ct`` (<TT>float</TT>):\n",
    "    The convergence tolerance used for policy or value iteration.\n",
    "    \n",
    "``gamma`` (<TT>float</TT>):\n",
    "    The discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the learn_strategy method, but read through its code\n",
    "def learn_strategy(planning_type = VALUE_ITER, max_iter = 10, print_every = 5, ct = None, gamma = 0.7):\n",
    "    # Loop over some number of episodes\n",
    "    V = np.zeros(num_states)\n",
    "    pi = np.zeros(num_states)\n",
    "\n",
    "    # Update Q-table using value/policy iteration until max iterations or until ct reached\n",
    "    for n_iter in range(max_iter):\n",
    "        V_prev = V.copy()\n",
    "\n",
    "        # Update V and pi using value or policy iteration.\n",
    "        if planning_type == VALUE_ITER:\n",
    "            V, pi = update_value_iteration(V, pi, gamma)\n",
    "        elif planning_type == POLICY_ITER:\n",
    "            V = policy_evaluation(pi, gamma)\n",
    "            pi = update_policy_iteration(V, gamma)\n",
    "        \n",
    "        # Calculate the difference between this V and the previous V\n",
    "        diff = np.absolute(np.subtract(V, V_prev))\n",
    "\n",
    "        # Check that every state's difference is less than the convergence tol\n",
    "        if ct and np.max(diff) < ct:\n",
    "            make_value_plot(V = V)\n",
    "            make_policy_plot(pi = pi, iter_type = planning_type, iter_num = n_iter+1)\n",
    "            print(\"Converged at iteration \" + str(n_iter+1))\n",
    "            return 0\n",
    "\n",
    "        # Make value plot and plot the policy\n",
    "        if (n_iter % print_every == 0):\n",
    "            make_value_plot(V = V)\n",
    "            make_policy_plot(pi = pi, iter_type = planning_type, iter_num = n_iter+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c0d7e",
   "metadata": {},
   "source": [
    "## Show Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Beginning policy iteration.')\n",
    "learn_strategy(planning_type=POLICY_ITER, max_iter = 4, print_every = 1, ct = 0.01, gamma = 0.7)\n",
    "print('Policy iteration complete.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46684550",
   "metadata": {},
   "source": [
    "## Show Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Beginning value iteration.')\n",
    "learn_strategy(planning_type=VALUE_ITER, max_iter = 4, print_every = 1, ct = 0.01, gamma = 0.7)\n",
    "print('Value iteration complete.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485075cf-3982-4550-ac7b-9b089bf4f77c",
   "metadata": {},
   "source": [
    "# Problem 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb97f54-0a1c-45ad-aad8-d49dfeb3cb8a",
   "metadata": {},
   "source": [
    "Make sure to install pygame (i.e. through running `pip install pygame`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d9b70-f73b-4e43-86a9-09efdc324ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pygame as pg\n",
    "\n",
    "# uncomment this for animation\n",
    "# from p3src.SwingyMonkey import SwingyMonkey\n",
    "\n",
    "# uncomment this for no animation (use this for most purposes! it gets very slow otherwise)\n",
    "from p3src.SwingyMonkeyNoAnimation import SwingyMonkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3c8ad-7c0b-4d32-98f5-e04b360b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants. Don't edit this!\n",
    "X_BINSIZE = 200\n",
    "Y_BINSIZE = 100\n",
    "X_SCREEN = 1400\n",
    "Y_SCREEN = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44124dab-3688-4e56-89fe-3733a8c1732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomJumper(object):\n",
    "    \"\"\"\n",
    "    This agent jumps randomly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "\n",
    "        # We initialize our Q-value grid that has an entry for each action and state.\n",
    "        # (action, rel_x, rel_y)\n",
    "        self.Q = np.zeros((2, X_SCREEN // X_BINSIZE, Y_SCREEN // Y_BINSIZE))\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"\n",
    "        Discretize the position space to produce binned features.\n",
    "        rel_x = the binned relative horizontal distance between the monkey and the tree\n",
    "        rel_y = the binned relative vertical distance between the monkey and the tree\n",
    "        \"\"\"\n",
    "\n",
    "        rel_x = int((state[\"tree\"][\"dist\"]) // X_BINSIZE)\n",
    "        rel_y = int((state[\"tree\"][\"top\"] - state[\"monkey\"][\"top\"]) // Y_BINSIZE)\n",
    "        return (rel_x, rel_y)\n",
    "\n",
    "    def action_callback(self, state):\n",
    "        \"\"\"\n",
    "        Implement this function to learn things and take actions.\n",
    "        Return 0 if you don't want to jump and 1 if you do.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO (currently monkey just jumps around randomly)\n",
    "        # 1. Discretize 'state' to get your transformed 'current state' features.\n",
    "        # 2. Perform the Q-Learning update using 'current state' and the 'last state'.\n",
    "        # 3. Choose the next action using an epsilon-greedy policy.\n",
    "\n",
    "        new_action = npr.rand() < 0.1\n",
    "        new_state = state\n",
    "\n",
    "        self.last_action = new_action\n",
    "        self.last_state = new_state\n",
    "\n",
    "        return self.last_action\n",
    "\n",
    "    def reward_callback(self, reward):\n",
    "        \"\"\"This gets called so you can see what reward you get.\"\"\"\n",
    "\n",
    "        self.last_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d979f-59a8-453c-8e2c-1475bc3ad4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    \"\"\"\n",
    "    Implement this bot! A good start is to look at the skeleton of RandomJumper, which \n",
    "    frames the bot in a good way, but has poor learning logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def action_callback(self, state):\n",
    "        \"\"\"\n",
    "        Whenever the state changes, this function will be called.\n",
    "        Return 0 if you don't want to jump and 1 if you do.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reward_callback(self, reward):\n",
    "        \"\"\"\n",
    "        When you are given a reward, this function is called.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a2b9f-e043-44a7-ab15-fa5ab52b5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_games(learner, hist, iters=100, t_len=100):\n",
    "    \"\"\"\n",
    "    Driver function to simulate learning by having the agent play a sequence of games.\n",
    "    \"\"\"\n",
    "    for ii in range(iters):\n",
    "        # Make a new monkey object.\n",
    "        swing = SwingyMonkey(sound=False,  # Don't play sounds.\n",
    "                             text=\"Epoch %d\" % (ii),  # Display the epoch on screen.\n",
    "                             tick_length=t_len,  # Make game ticks super fast.\n",
    "                             action_callback=learner.action_callback,\n",
    "                             reward_callback=learner.reward_callback)\n",
    "\n",
    "        # Loop until you hit something.\n",
    "        while swing.game_loop():\n",
    "            pass\n",
    "\n",
    "        # Save score history.\n",
    "        hist.append(swing.score)\n",
    "\n",
    "        # Reset the state of the learner.\n",
    "        learner.reset()\n",
    "    pg.quit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c482628-4eaf-4be5-a57a-332d5e436edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the agent you want to run.\n",
    "# agent = Learner()\n",
    "agent = RandomJumper()\n",
    "\n",
    "# Empty list to save history.\n",
    "hist = []\n",
    "\n",
    "# Run games. You can update t_len to be smaller to run it faster.\n",
    "run_games(agent, hist, 100, 100)\n",
    "print(hist)\n",
    "\n",
    "# Save history. \n",
    "np.save('hist', np.array(hist))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs238]",
   "language": "python",
   "name": "conda-env-cs238-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
