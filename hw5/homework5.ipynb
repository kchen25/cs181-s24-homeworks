{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 181 HW5**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize data and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a specific example of when we have $K = 3$ component Gamma distributions. Let's initialize the initial parameter values for $\\theta$ and $\\beta_k$ as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\theta_k &=  1/K, \\\\\n",
    "  \\beta_k & = k/K.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that we usually initialize $\\theta$ and $\\beta_k$ randomly. However, by fixing the initial $\\theta$ and $\\beta_k$, EM becomes deterministic which makes debugging (and grading) easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "x = torch.load('data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to use numpy (optional)\n",
    "# import numpy as np\n",
    "# from scipy.stats import gamma\n",
    "# x = x.numpy()\n",
    "# theta = theta.numpy()\n",
    "# betas = betas.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the E-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(theta, betas):\n",
    "    q = 'not implemented'\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the M-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(q):\n",
    "    theta_hat = 'not implemented'\n",
    "    beta_hats = 'not implemented'\n",
    "    return theta_hat, beta_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_px(x, theta, betas):\n",
    "    p = 'not implemented'\n",
    "    return p\n",
    "\n",
    "def log_likelihood(theta, betas):\n",
    "    return log_px(x, theta, betas).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_em(theta, betas, iterations=1000):\n",
    "    theta = 'not implemented'\n",
    "    betas = 'not implemented'\n",
    "    return theta, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overlay_plot(theta, betas):\n",
    "    x_test = torch.linspace(0.01, x.max(), 1000)\n",
    "    prob = log_px(x_test.unsqueeze(-1), theta, betas).exp()\n",
    "    # prob = np.exp(log_px(x_test.unsqueeze(-1), theta, betas))  # use this line for numpy\n",
    "    ll = log_likelihood(theta, betas)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    fig.subplots_adjust(top=0.7)\n",
    "    fig.suptitle(f'theta = {theta}\\nbeta = {betas}\\nlog likelihood = {ll:.3e}')\n",
    "    \n",
    "    ax.hist(x.T, bins=100, color='tomato', alpha=0.5, density=True, label='Dataset')\n",
    "    ax.plot(x_test, prob, color='royalblue', label='Gamma mixture')\n",
    "    \n",
    "    ax.set_title(f'Dataset and Gamma mixture (K={len(theta)})')\n",
    "    ax.set_xlabel('Recovery time (hours)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 5.0\n",
    "for K in range(1,5):\n",
    "    theta0 = torch.ones(K) / K\n",
    "    betas0 = (torch.arange(K) + 1) / K\n",
    "    # theta0 = np.ones(K) / K               # use this for numpy\n",
    "    # betas0 = (np.arange(K) + 1) / K\n",
    "    \n",
    "    theta, betas = run_em(theta0, betas0)\n",
    "    \n",
    "    make_overlay_plot(theta, betas)\n",
    "    plt.savefig(f'p2_3_{K}mixtures.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)  # download MNIST\n",
    "N = 6000 \n",
    "\n",
    "x = mnist_trainset.data[:N]  # select N datapoints\n",
    "x = x.flatten(1)             # flatten the images\n",
    "x = x.float()                # convert pixels from uint8 to float\n",
    "# x = x.numpy()              # uncomment to use numpy (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement PCA\n",
    "\n",
    "*Hint: see `.linalg.svd()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x, n_comps=500):\n",
    "    top_eigvals = 'not implemented'\n",
    "    top_pcomps = 'not implemented'\n",
    "    return top_eigvals, top_pcomps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** calculate cumulative fraction of variance\n",
    "\n",
    "*Hint: see `.cumsum()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cfvs(eigvals):\n",
    "    cum_frac_vars = 'not implemented'\n",
    "    return cum_frac_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** calculate mean squared L2 norm reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_errs(x, pcomps):\n",
    "    err_mean = 'not implemented'\n",
    "    err_pcomp = 'not implemented'\n",
    "    return err_mean, err_pcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and print errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pic(pic, ax, title=''):\n",
    "    x = pic.reshape(28, 28)\n",
    "    ax.imshow(x, cmap='binary')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "def make_plots(eigvals, cfvs, x_mean, pcomps):\n",
    "    # plot eigenvals and cfvs\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax1.plot(eigvals, color='tomato')\n",
    "    ax1.set_title('Eigenvalues')\n",
    "    ax2.plot(cfvs, color='tomato')\n",
    "    ax2.set_title('CFVs')\n",
    "    fig.savefig('p3_cfvs.pdf')\n",
    "\n",
    "    # plot mean\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    plot_pic(x_mean, ax, title='Mean')\n",
    "    fig.savefig('p3_mean.pdf')\n",
    "\n",
    "    # plot top 10 pcomps\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i in range(10):\n",
    "        plot_pic(pcomps[i], axes.flat[i], title=f'PC index {i}')\n",
    "    fig.savefig('p3_pcomps.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do PCA\n",
    "eigvals, pcomps = pca(x)\n",
    "\n",
    "# calculate CFVs\n",
    "fcvs = calc_cfvs(eigvals)\n",
    "\n",
    "# print errors\n",
    "err_mean, err_pcomp = calc_errs(x, pcomps)\n",
    "print(f'Reconstruction error (using mean): {err_mean:3e}')\n",
    "print(f'Reconstruction error (using mean and top 10 pcomps): {err_pcomp:3e}')\n",
    "\n",
    "# make plots\n",
    "make_plots(eigvals, fcvs, x.mean(0), pcomps)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c365ea26a318e0b540d1978e97e3d03cfe51dff8cd04dae5f3d7b47d79d2453"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
